\section{Cache}
Cache serves as an intermediate memory buffer between the CPU and DRAM. It is faster than DRAM ($\sim$3-50 ns vs.\ $\sim$100 ns) but much smaller in size.
In general, cache can save the same type of memory as is stored in DRAM.
\subsection{Principles}
\ptitle{Hits and Misses}

If the required memory is found in cache the memory can be taken from there (\textbf{cache hit}, fast), else the memory has to be loaded from DRAM (\textbf{cache miss}, slow).

\newpar{}
\ptitle{Locality}

The principles of locality are used to increase the cache hit rate:
\begin{itemize}
    \item \textbf{Principle of temporal locality (PTL)}:
          \begin{itemize}
              \item Information that was accessed recently, will likely be accessed soon again
              \item Example: variables in a for looop
          \end{itemize}
    \item \textbf{Priciple of spatial locality (PSL)}
          \begin{itemize}
              \item Information that is close in memory space, will be accessed together.
              \item Examples: Array, linked lists etc.\
          \end{itemize}
\end{itemize}

\subsection{Page Translation}
To decrease the computational cost arising from page table walks, a \textit{translation lookaside buffer} can be used to store frequently accessed page table entries.

\subsubsection{Translation Lookaside Buffer (TLB)}
\begin{itemize}
    \item TLB hit $\to$ return TLB entry (physical address)
    \item TLB miss $\to$ page table walk $\to$ save found physical page to TLB
\end{itemize}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{9pt}

\begin{tabularx}{\linewidth}{@{}cc@{}}
    \multicolumn{2}{c}{\textbf{\code{TLB}}} \\
    \cmidrule{1-2}
    virt.\ address & phys.\ address         \\
    \cmidrule{1-2}
    $\vdots$       & $\vdots$               \\
    \cmidrule{1-2}
    virt.\ address & phys.\ address         \\
    \cmidrule{1-2}
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\newpar{}
\ptitle{Eviction}

If the TLB is full entries can be evicted using the \textit{least recently used} (LRU) strategy. This strategy complies with PTL.

\newpar{}
\textbf{Remarks}
\begin{itemize}
    \item The TLB is only valid in one virtual address space, i.e.\ the TLB needs to be flushed explicity when changing vmas!
    \item Storing larger pages, i.e.\ huge pages in the TLB improves the coverage of the TLB. This idea is guided by PSL.
    \item Typical sizes: $\mathcal{O}(10)$ in small CPUs, $\mathcal{O}(100)$ in larger ones.
\end{itemize}

\subsection{Data/ Instruction Cache}

\subsubsection{Structure}
\ptitle{Cache Line Size}

In DDR4 a cache line has due to \textit{read/write bursts} the size
\noindent\begin{equation*}
    \underbrace{8}_{\textsf{pipelines}}\cdot \underbrace{8~\mathrm{byte}}_{\textsf{request size}} = 64 ~\mathrm{byte}
\end{equation*}

\begin{itemize}
    \item \textbf{larger} cache lines would reduce the number of cache lines in the cache (violates PTL)
    \item \textbf{smaller} cache lines would reduce the number of data accessed at once (violates PSL)
\end{itemize}

\newpar{}
\textbf{Remarks}
\begin{itemize}
    \item In contrast to TLB cache, data/instruction cache is accessed with \textbf{physical} addresses to avoid flushing the cache when switching vmas.
    \item In the case of TLB, the data remain in DRAM but the address translation is saved in cache. For data/instruction cache the actual data are cached using their physical addresses.
\end{itemize}

\subsubsection{Set-Associative Cache}
In set-associative cache, a DRAM entry can go to any slot in their set. The \textbf{wayness} is the number of entries per set (e.g.\ 4-way 4-set associative).

\ptitle{Wayness-Set Tradeoff}
\begin{itemize}
    \item \textbf{direct-mapped} (1-way n-set associative) fast lookup, high eviction rate. %cache, one has $\mathcal{O}(1)$ lookup but the chance for eviction is higher as there is no way to dodge a conflict because only 1 cache slot is assigned per DRAM block.
    \item \textbf{fully-associative} (n-way 1-set associative) slow lookup, low eviction rate. %cache one has $\mathcal{O}(n)$ lookup but the chance for eviction is small as one can dodge conflicts.
    \item A \textbf{set-associative} cache finds a tradeoff between risk of eviction and lookop time such that, on typical workloads, most accesses do not cause eviction.
    \begin{itemize}
        \item Has always less slot bits than direct-mapped cache.
        \item \textit{Within} a set, LRU is commonly used on eviction.
    \end{itemize}
\end{itemize}

\paragraph{Direct Mapped Cache}
In direct-mapped cache, every slot of the cache can only map to one place in DRAM.
\begin{itemize}
    \item Tag is the ``row'' in DRAM, ``slot'' the column.
    \item The location in cache is solely determined by the slot. However, tag must be stored also to have a full reference to DRAM.
\end{itemize}
\newpar{}
The 64-bit cache line consists of
\begin{itemize}
    \item \textbf{Offset} within the cache line (6 bits: \fncode{cccccc})
    \item \textbf{Slot} (for 16/$2^4$ slots we need 4 bits: \fncode{ssss})
    \item \textbf{Tag} (22 bits: \fncode{tt..tt})
\end{itemize}
\includegraphics[width = \linewidth]{Cache.png}
% TODO: The "cash miss" entry in the picture has tag 1 instead of >>1
\newpar{}
\ptitle{Properties}
\begin{itemize}
    \item[+] simple design
    \item[+] $\mathcal{O}(1)$: check one tag to detect hit/miss
    \item[-] sparse population
    \item[-] violating PTL: a cache line can be evicted even though there is space left in the cache.
\end{itemize}

\paragraph{Fully-Assiociative Cache}
With fully-assiociative cache, every line from DRAM can go to every cache entry. 
\begin{itemize}
    \item There are no slot bits anymore.
    \item Very small caches (e.g.\ small TLBs) use this approach.
    \item Use LRU policy on eviction (PTL) as we ignore spatial context.
\end{itemize}

\newpar{}
The 64-bit cache line consists of
\begin{itemize}
    \item \textbf{Offset} within the cache line (6 bits: \fncode{cccccc})
    \item \textbf{Tag} (26 bits: \fncode{tt..tt})
\end{itemize}

\newpar{}
\ptitle{Properties}
\begin{itemize}
    \item[+] Flexible
    \item[+] Dense population
    \item[-] $\mathcal{O}(n)$ for lookup
    \item[-] Many tag bits
    \item[-] Search through all tags
\end{itemize}

\subsubsection{Loading vs. Storing}
\ptitle{Loading}
\begin{itemize}
    \item If available, the memory can be read directly from cache
    \item Cached loads result in \textbf{clean} cache lines
\end{itemize}

\newpar{}
\ptitle{Storing}
\begin{itemize}
    \item Memory is first stored in cache alone, resulting in inconsistencies between cache and DRAM (\textbf{dirty} cache lines)
    \item On eviction
          \begin{itemize}
              \item \textbf{clean} cache lines can be discarded
              \item \textbf{dirty} cache lines need to be written to DRAM before discarding:
                    \begin{itemize}
                        \item \textbf{write-back} policy:\newline write dirty cache line at \textbf{eviction time}
                        \item \textbf{write-through} policy:\newline write dirty cache line at \textbf{store time}
                        \item \textbf{write-background} policy:\newline write dirty cache line in the \textbf{background}
                    \end{itemize}
          \end{itemize}
\end{itemize}