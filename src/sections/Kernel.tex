\section{Kernel}
Privileged software that is in charge of securely multiplexing hardware resources between competing processes.

\subsection{Physical Memory Management}
\begin{itemize}
    \item Memory is usually DRAM (capacitors that need to be refreshed periodically)
    \item Last parallel bus (64/72 Bit) in modern computers
\end{itemize}


\subsubsection{Memory Sections}
\includegraphics[width = \linewidth]{memory_sections.png}

Kernel allocates code/stack at program start. Heap is allocated dynamically by the kernel and managed by the heap allocator library (e.g.\ \textit{heap.so}, part of application).

\ptitle{.text}

\begin{itemize}
    \item Generated by compiler
    \item Code (machine instructions)
    \item Does not change after the program ist written (except browser)
\end{itemize}


\ptitle{.data/.bss}

\begin{itemize}
    \item Generated by compiler
    \item \textit{.data}: initialized global vars, static function vars (sizes known beforehand)
    \item \textit{.bss}: same as above but not initialized: allocate a zero page and point to it (no physical memory usage)
\end{itemize}

\ptitle{Stack}

\begin{itemize}
    \item Compiler manages fixed-sized stack
    \item Local variables, callee-saved registers, return address
\end{itemize}

\ptitle{Heap}
\begin{lstlisting}[language={C}]
    void *ptr = malloc(size_t);
    free(ptr);                  
\end{lstlisting}
\begin{itemize}
    \item User steers lifetime (new, delete)
    \item Available across functions
    \item Used also for large allocations
\end{itemize}

\subsubsection{Memory Allocators in Jake}
\begin{center}
    \includegraphics[width=\linewidth]{Allocators.png}
\end{center}

\subsubsection{Dynamic Memory Management (Heap)}
\begin{itemize}
    \item Heap suffers \textbf{fragmentation} at runtime: causes non-contiguous memory
          \noindent\begin{itemize}
              \item \textbf{External fragmentation} is caused by disadvantageous sequences of allocations/freeings (there would be enough free heap but it is not contiguous)
              \item \textbf{Internal fragmentation} is caused by having large heap units where small allocations waste memory
          \end{itemize}
          % \item Fragmentation and finding free memory must be handled
    \item \textbf{Heap metadata} are used to keep track of free memory and for sanity checks
          \noindent\begin{itemize}
              \item \textbf{In-band heap metadata} are stored inside the heap
              \item \textbf{Out-of-band heap metadata} are kept outside the heap
          \end{itemize}
\end{itemize}

\subsubsection{Linked List (LL): In-Band Metadata}

\ptitle{Data Structure}
\begin{lstlisting}[language={C}]
    typedef struct __node_t {
        int size;                           // allocation size
        union {                             // contains either:
            int magic;                      // sanity check or
            struct __node_t *next;          // next free section
        } u;
    } node_t;         
    \end{lstlisting}

\includegraphics[width = \linewidth]{free_linked_list.png}

\begin{itemize}
    \item Nodes (8 B) stored at start of each free heap section contain information about size and status
    \item Use a LL to keep track of \textbf{free} heap sections
    \item External fragmentation but no internal fragmentation
\end{itemize}

\newpar{}
\ptitle{Malloc}
\begin{enumerate}
    \item Walk the linked list to find free space
    \item set magic value und remove node from free LL (update pointers)
    \item return pointer to allocated heap
\end{enumerate}

\newpar{}
\begin{itemize}
    \item \textbf{First fit}: take first free slot that is big enough (more space efficient <- small losses at each allocation!) % TODO: check if this is correct
    \item \textbf{Best fit}: take slot matching the required size the best (slow: walk entire list)
\end{itemize}

\newpar{}
\ptitle{\code{free(a)}}%ChkTex 36

\begin{enumerate}
    \item Check magic value. If it has been overwritten: out-of-bound software bug
    \item Allocate a new free node
    \item Insert node to the front of the free LL:
          \begin{itemize}
              \item \code{*next = head}
              \item \code{head = node}
          \end{itemize}
\end{enumerate}
A \textit{doubly linked list} can be used for convenient unlinking

\newpar{}
\ptitle{Handling Fragmentation}

\begin{itemize}
    \item Coalesce/ combine the free list
    \item Walk the list, find adjacent free items and merge them
\end{itemize}

\subsubsection[Bitmap-Based Allocations]{Bitmap-Based Allocations: Out-Of-Band Metadata}
\begin{itemize}
    \item Every bit can tell whether $N$ bytes of the heap are free
          \begin{itemize}
              \item Can be more wasteful than LL metadata: inflexible block size
              \item Internal fragmentation    % TODO: In the book it says that variable-size allocations in bitmap allocators cause internal fragmentation. But isn't there also external fragmentation in bitmap allocators? % Answer: I think theres also external fragmentation but its less dramatic than in LL.
          \end{itemize}
    \item One needs \textbf{additional} data to keep track of the total allocation sizes (as we only know status of our $N$ byte units)
    \item \code{malloc()} scans the bitmap for free space %ChkTex 36
          \begin{itemize}
              \item Update allocation size in additional metadata
          \end{itemize}
    \item \code{free()} flips the right bits in the bitmap to zero %ChkTex 36
          \begin{itemize}
              \item Get size to free from additional metadata
              \item Coalescing comes for free
          \end{itemize}
\end{itemize}

\newpar{}
\ptitle{Bitmap Granularity}

\begin{itemize}
    % \item Coalescing is very cheap (flipping bits) % done automatically
    \item Depending on the bitmaps granularity, there is a tradeoff between \textbf{fragmentation} and \textbf{performance}:%ChkTex 36
          \begin{itemize}
              \item \textbf{fine bitmap}: low internal fragmentation but expensive \code{malloc()} %ChkTex 36
              \item \textbf{coarse bitmap}:high internal frag but cheap \code{malloc()} %ChkTex 36
          \end{itemize}
\end{itemize}


\subsubsection{Slab Allocatior: In-/Out-Of-Band Metadata}

\begin{itemize}
    \item Create ``sub-heaps'' (slabs) for common allocation sizes to reduce ext./int.\ fragmentation
    \item Each slab contains e.g. 12/16$\dots$ 256-byte entries
          %%% The following bullet points are not specific to slab allocation and are featured in section "memory allocators in Jake"
          % \item Applications' allocators ask kernel for memory
          % \item Built \textit{on top} of low-level allocator:
          %       \begin{itemize}
          %           \item Kernel (buddy allocator) provides memory and can grow full slabs / shrink empty ones
          %           \item Slab allocator for small memory requests within pages
          %       \end{itemize}
    \item Implementation:
          \begin{itemize}
              \item LL: No coalescing, fast malloc but extra metadata
              \item Bitmap: Single bit metadata but slower malloc due to scanning
          \end{itemize}
\end{itemize}

\ptitle{Pros and Cons}
\begin{itemize}
    \item[+] Flexible (various allocation sizes, growing/shrinking)
    \item[-] Some internal fragmentation
    \item[-] External fragmentation: \textit{Fragmentation across slabs} (not optimized for full utilization): Can't move memory between slabs as they become full/empty
          %   \begin{itemize}
          %       \item Solution: Use page allocation to move pages between different slabs (Buddy Paging)
          %   \end{itemize}
\end{itemize}

\subsubsection{Page Allocation: Buddy Allocator}
Page allocation optimizes for \textbf{full utilization}.
\begin{itemize}
    \item A page is a \textbf{contiguous area of memory} aligned to the page size (4 KB)
    \item Blocks can have a size that scales with power-of-two wrt.\ the page size, this power is also called the \code{order}.
          \begin{itemize}
              \item Code, stack, heap are at least 1 page each.
          \end{itemize}
    \item Each page has a \textit{buddy} with the same size.
          \begin{itemize}
              \item If a page is split, a new page and a buddy with \code{order - 1} are generated.
              \item If a page is freed and its buddy is also free, these two pages are merged into a page with \code{order + 1}.
          \end{itemize}
    \item Page allocation finds tradeoff between performance and efficient memory usage
\end{itemize}

\paragraph{Buddy Allocator: Interface}
\ptitle{\code{block\_alloc(order)}} %ChkTex 36
\begin{enumerate}
    \item if no block of \code{order} is available, split higher order block recursively.
    \item allocate block of desired \code{order}
\end{enumerate}

Implementation:
\begin{lstlisting}[style=bright_C++]
struct block *buddy_alloc(unsigned order)
{
    if (order > BUDDY_MAX_ORDER) return NULL;
    int smallest_order = __buddy_find_smallest_free_order(order);
    if (smallest_order == -1) return -1;

    int ret = buddy_split(smallest_order, order);
    if(ret == -2) return NULL;

    return buddy_pop(order);
}
\end{lstlisting}

\newpar{}
\ptitle{\code{block\_free(block)}} %ChkTex 36
\begin{enumerate}
    \item free block
    \item if buddy is free, merge into larger block
\end{enumerate}

Implementation:
\begin{lstlisting}[style=bright_C++]
int buddy_free(struct block *block)
{
    switch (block->refcnt) {
    case 0:
        return -1; /* Double free */
    case 1:
        buddy_push(block, block->order);
        __buddy_try_merge(block);

        return 0;
    default:
        block->refcnt--;
        return 0;
    }
}
\end{lstlisting}

\textbf{Remarks}
\begin{itemize}
    \item Both \code{block\_alloc} and \code{block\_free} trigger a max of (\code{max\_order}) of operations. %ChkTeX 36 
    \item Initialization: Call \code{block\_free} on every 4kB page (causes merging).
\end{itemize}

\paragraph{Buddy Allocator: Data Structures}
\ptitle{Block}
\begin{lstlisting}[style=bright_C++]
struct block {
    unsigned refcnt;    /* e.g. for shared memory */
    unsigned order; /* 0 <= order <= BUDDY_MAX_ORDER */
    struct block *next;
};
\end{lstlisting}
\newpar{}
\begin{lstlisting}[style=bright_C++]
static struct block buddy_blocks[1UL << BUDDY_MAX_ORDER];
\end{lstlisting}

\ptitle{Free List}
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_free_lists[BUDDY_MAX_ORDER + 1];
\end{lstlisting}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item Static heap size assumed in Jake: \texttt{buddy\_blocks} in \texttt{.bss}
    \item Static assumption not used in general OS (variable heap size per system)
          \begin{itemize}
              \item Use bootmem allocator
          \end{itemize}
    \item \texttt{buddy\_free\_lists} to avoid bitmap scans: cheap malloc
\end{itemize}

\paragraph{Buddy Allocator: Internal Functions}
\ptitle{Block to Buddy}
\begin{lstlisting}[style=bright_C++]
static inline struct block *block2buddy(struct block *block, int order)
{
    return __ppn2block(block2__ppn(block) ^ 1UL << order);
}
\end{lstlisting}
Note: If address instead of page number is used: flip \texttt{12+order+1}th bit.

\newpar{}
\ptitle{Smallest Order}
\begin{lstlisting}[style=bright_C++]
static int __buddy_find_smallest_free_order(unsigned order)
{
    for (int i = order; i <= BUDDY_MAX_ORDER; i++) {
        if (!is_list_empty(i)) return i;
    }
    return -1;
}
\end{lstlisting}

\paragraph{Buddy Allocator: Managing the Free LL}
\ptitle{Push}

Push to the front of \code{buddy\_free\_lists}
\begin{lstlisting}[style=bright_C++]
static void buddy_push(struct block *block, unsigned order)
{
	block->next = buddy_free_lists[order];
	buddy_free_lists[order] = block;
	block->refcnt--;
}
\end{lstlisting}

\newpar{}
\ptitle{Pop}

Pop from the front of \code{buddy\_free\_lists}
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_pop(unsigned order)
{
    if (is_list_empty(order)) return NULL;

    struct block *block = buddy_free_lists[order];

    /* update list and pop front of linked list */
    if (block->next != NULL) {
        buddy_free_lists[order] = block->next;
        block->next = NULL;
    } else {
        buddy_free_lists[order] = NULL;
    }
    block->refcnt++;

    return block;
}
\end{lstlisting}

\newpar{}
\ptitle{Remove}

Find \code{block} in  \code{buddy\_free\_lists}, move to front and \code{pop}.
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_remove(struct block *block, unsigned order)
{
	if (is_list_empty(order)) return NULL;

	/* Find block in linked list and bring it to the front */
	if (buddy_free_lists[order] != block) {
		struct block *head = buddy_free_lists[order];
		struct block *tail = head;
		struct block *prev = NULL;

		while (tail->next) {
			if (tail->next == block) prev = tail;
			tail = tail->next;
		}

		if (!prev) return NULL; /* Could not find block */

		buddy_free_lists[order] = block;
		prev->next = NULL;
		tail->next = head;
	}

	return buddy_pop(order);
}
\end{lstlisting}

\paragraph{Buddy Allocator: Moving Memory Accross Levels}
\ptitle{Split}
\begin{lstlisting}[style=bright_C++]
static int buddy_split(unsigned smallest_free_order, unsigned desired_order)
{
	/* Will do nothing if smallest_free and desired are equal */
	for (int i = smallest_free_order; i > desired_order; i--) {
		int ret = __buddy_split(i);
		if (ret) return ret;
	}
	return 0;
}
\end{lstlisting}

\newpar{}
\begin{lstlisting}[style=bright_C++]
static int __buddy_split(unsigned order)
{
    if (order == 0) return -1;

    struct block *block = buddy_pop(order);

    if (!block) return -2; // no block available

    /* create buddy */
    block->order--;
    struct block *buddy = block2buddy(block, block->order);
    buddy->order = block->order;

    /* push split blocks to free lists */
    buddy_push(block, block->order);
    buddy_push(buddy, buddy->order);

    return 0;
}
\end{lstlisting}

\newpar{}
\ptitle{Merge}
\begin{lstlisting}[style=bright_C++]
static void __buddy_try_merge(struct block *block)
{
    if (block->order == BUDDY_MAX_ORDER) return;

    /* Get buddy of block */
    struct block *buddy = block2buddy(block, block->order);

    if (block->order != buddy->order) return;

    if (is_block_free(block) && is_block_free(buddy)) {
        block = __buddy_merge(block, buddy);
        __buddy_try_merge(block);
    }
}
\end{lstlisting}
\newpar{}
\begin{lstlisting}[style=bright_C++]
static struct block *__buddy_merge(struct block *block, struct block *buddy)
{
    /* Remove them from current order's list */
    buddy_remove(buddy, buddy->order);
    buddy_remove(block, block->order);

    /* Merge into the one with the smaller address */
    if (block2__ppn(block) < block2__ppn(buddy)) {
        block->order++;
        buddy_push(block, block->order);
        return block;
    } else {
        buddy->order++;
        buddy_push(buddy, buddy->order);
        return buddy;
    }
}
\end{lstlisting}

%% Redundant with the diagram in Section "Memory allocators in Jake"
% \paragraph{In-Application Memory Management}

% \begin{itemize}
%     \item Applications request memory from the kernel.
%     \item Then, internally, they have themselves a variety of allocators to manage it.
% \end{itemize}

\subsection{Virtual Memory}
% Segmentation and x86 history left out on purpose
With virtual memory, the users contiguous memory is spread out in physical memory to improve flexibility and external fragmentation.
In order to achieve this, the memory management unit (\textbf{MMU}) translates memory addresses accordingly to create the illusion of contiguous memory for the user. In addition the MMU also controls the permissions to that virtual memory space.

\subsubsection{Memory Access in Jake}
\begin{center}
    \includegraphics[width=0.9\linewidth]{memory_access.png}
\end{center}

\subsubsection{Segmentation}

\begin{itemize}
    \item Inflexible
    \item Suffers from both internal and external fragmentation
\end{itemize}

\subsubsection{Paging}
Solves issues arising from segmentation.

\begin{center}
    \includegraphics[width = \linewidth]{kernel_virt_mem.png}
\end{center}

\ptitle{Page Table}
\begin{itemize}
    \item translates the virtual page into a physical one
    \item In RISC-V, Pages have 4 KB (12 bit) granularity
          \begin{itemize}
              \item Reason for buddy page size
          \end{itemize}
    \item Kernel allocates pages (buddy)
    \item CPU's MMU translates addresses using the page table
\end{itemize}

\paragraph{Linear Page Table}
\begin{itemize}
    \item stored in CPU
    \item large unused parts (virtual $>>$ physical)
          \begin{itemize}
              \item wastes physical memory
          \end{itemize}
    \item simple
    \item virtual address is index to page table
          \begin{itemize}
              \item virtual offset = physical offset within page
          \end{itemize}
\end{itemize}

\ptitle{Dimensions}
\noindent\begin{equation*}
    s_{\text{page table}} = n_{\text{virt}}\cdot (\log_2(n_{\text{phys}}) + 1)\quad [\text{bits}]
\end{equation*}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}
\begin{tabularx}{\linewidth}{@{} c c c@{}}
                                                                                   & physical page                  & present bit \\
    \cmidrule{2-3}
    \multirow{7}{*}{\begin{sideways}$n_{\text{virt}}$\end{sideways}} & 10                             & 1           \\
                                                                                   & 01                             & 1           \\
                                                                                   & x                              & 0           \\
                                                                                   & x                              & 0           \\
                                                                                   & 00                             & 1           \\
                                                                                   & 11                             & 1           \\
                                                                                   & x                              & 0           \\
                                                                                   & x                              & 0           \\
    \cmidrule{2-3}
                                                                                   & $\log_2(n_{\text{phys}})$ bits &
\end{tabularx}

\paragraph{Multi-level Page Tables}
\begin{itemize}
    \item Pages are allocated when needed
    \item Stored im DRAM
    \item In our case (\textbf{Sv48}):
          \begin{itemize}
              \item 48 bit virtual and 56 bit physical address space
              \item 4 levels
              \item page table entry (PTE) is 8 bytes (64 bit)
              \item page is 4 KB $\to \frac{\text{4 KB}}{\text{8 B / entry}} = 512$ entries
                    \begin{itemize}
                        \item $\log_{2}(512) = 9$ bits required to index page table to get a PTE
                    \end{itemize}
              \item virtual page number: 4 (levels) $\cdot$ 9 bits = 36 bits
                    \begin{itemize}
                        \item the remaining $48-36=12$ bits are used to index the ${2}^{12} = 4$ KB of physical bytes
                    \end{itemize}
          \end{itemize}
\end{itemize}

\begin{center}
    \includegraphics[width =\linewidth]{kernel_sv48.png}
\end{center}

\ptitle{Virtual $\to$ physical (memory walk)}
\begin{itemize}
    \item Done by CPU's MMU
\end{itemize}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_ml_mem_walk.png}
\end{center}

\newpar{}
\ptitle{Creation}
\begin{itemize}
    \item Done in software (by OS)
    \item Identical to memory walk.
    \item If \code{present == 0}, allocate new page (\code{buddy\_alloc(0)} $\to$ \code{ppn}) and update PPN.\ %ChkTeX 36
\end{itemize}

\ptitle{Page Table Entry}

\begin{center}
    \includegraphics[width = \linewidth]{kernel_page_entry.png}
\end{center}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}llcll@{}}
    V & valid     &  & G    & global (vma)  \\
    R & read      &  & A    & accessed      \\
    W & write     &  & D    & dirty (write) \\
    X & execute   &  & PBMT & IO            \\
    U & user mode &  & N    & TLBOPT
\end{tabularx}

\begin{tabularx}{\linewidth}{@{}cccl@{}}
    X & W & R & Meaning                             \\
    \cmidrule{1-4}
    0 & 0 & 0 & Pointer to next level of page table \\
    0 & 0 & 1 & Read-only page                      \\
    0 & 1 & 0 & Reserved for future use             \\
    0 & 1 & 1 & Read-write page                     \\
    1 & 0 & 0 & Execute-only page                   \\
    1 & 0 & 1 & Read-execute page                   \\
    1 & 1 & 0 & Reserved for future use             \\
    1 & 1 & 1 & Read-write-execute page
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\newpar{}
\ptitle{satp}
\begin{itemize}
    \item \code{satp} (supervisor address translation and protection) register points to the root of the page table
    \item \code{mode} turns on and specifies the MMU type
\end{itemize}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_satp.png}
\end{center}
\begin{center}
    \includegraphics[width = .6\linewidth]{kernel_satp_table.png}
\end{center}

\newpar{}
\ptitle{Bigger Page Types}

Every memory access requires 4 additional lookups (memory accesses). If the applications only need bigger junks of memory then the lookup tree can be pruned by a page (or more).

\newpar{}
One page table has 512 entries ($2^9$).
\begin{align*}
    \text{page}       & = & 4\text{~KB}         &   &               \\
    \text{Huge page}  & = & 512 * 4\text{~KB}   & = & 2\text{~MB}   \\
    \text{Super page} & = & 512^2 * 4\text{~KB} & = & 1\text{~GB}   \\
    \text{Giant page} & = & 512^3 * 4\text{~KB} & = & 512\text{~GB}
\end{align*}

To determine if the current page table entry points to a next page table or to a bigger page size the X, W, R flags in are used (all 0 = next level of page table). In case of e.g.\ a huge page the \code{V} bit is set at level 1 already instead of level 0.

\paragraph{The MMU}
\begin{itemize}
    \item The MMU is a HW component
    \item After starting it, MMU interposes on each memory access:
          \begin{itemize}
              \item Lookup TLB. If cache hit: done. If cache miss: continue
              \item Go to DRAM page at (phys.) address stored in \code{satp}
              \item Execute a memory walk to the physical address that corresponds to the given virtual address
              \item Store translation in TLB
          \end{itemize}
    \item During the memory walk the MMU
          \begin{itemize}
              \item Checks:
                    \begin{itemize}
                        \item validity using the \code{V} bit
                        \item permissions depending on the \code{R/W/X} bits
                        \item kernel/user permissions depending on the \code{U} bit
                              \begin{itemize}
                                  \item \texttt{sstatus} stores processor's state (kernel/user)
                              \end{itemize}
                    \end{itemize}
              \item Raises exceptions if validity or permissions are not given (page faults)
          \end{itemize}
          % \item Note that the kernel executes a similar PT walk in software when setting up/manipulating PTEs % Clearer formulated in page fault subsection
\end{itemize}

\subsubsection{Virtual Memory Management}
The virtual memory space is divided into two sections one for the application and one for the kernel.
\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_app_kernel_space.png}
\end{center}

The kernel memory space contains five regions:

\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_kernel_space.png}
\end{center}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}lXccc@{}}
    Section        & Description                         & X & W & R \\
    \midrule
    \texttt{I/O}   & Interaction with devices            & 0 & 1 & 1 \\
    \texttt{.text} & Kernel code                         & 1 & 0 & 0 \\
    \texttt{.data} & Initialized global/static variables & 0 & 1 & 1 \\
    \texttt{ID}    & Identity map of physical memory     & 0 & 1 & 1 \\
    \texttt{stack} & Kernel stack                        & 0 & 1 & 1
\end{tabularx}

\newpar{}
\ptitle{Remark}

\texttt{ID}
\begin{itemize}
    \item maps the entire physical memory left
    \item is required for manipulating page tables
          % TODO: Additional purposes?
\end{itemize}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\paragraph{MMU Start-Up}

For backward compatibility reasons the kernel is located in the upper part of the virtual memory. This introduces a problem with the PC when the MMU starts. First the PC is pointing to the physical address of the kernel code. But after the MMU initialized the virtual memory space the PC points to the wrong address.

\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_mmu_problem.png}
\end{center}

This is tackled by
\begin{enumerate}
    \item Double mapping the kernel before the MMU starts.
    \item Update pointers
          \begin{itemize}
              \item \code{sp, ra} etc.\ (heap)
          \end{itemize}
          and JALR to high address space
    \item Change the \code{mode} bit to turn on the MMU
    \item Remove the double mapped kernel.
\end{enumerate}

\subsubsection{VMAs}
% If an application violates the permissions or tries to access an unmapped memory area the CPU throws an exception. (For more details on exceptions see Section~\ref{exceptions}) This exception is called a page fault.
\ptitle{Data Structure}
\begin{lstlisting}[style=bright_C++]
struct vma{
    uintptr_t   vpn;        // start address
    size_t      pagen;
    unsigned    flags;      // permissions
    struct node node;
}
\end{lstlisting}

The kernel has a list of \textbf{virtual memory areas} (VMAs) for each process (and itself). VMAs are managed by the MMU (\code{mmap, munmap}).


\newpar{}
\ptitle{Benefits of Memory Regions}
\begin{itemize}
    \item \texttt{.code}
          \begin{itemize}
              \item apps have many functions: not all used at the same time.
              \item only allocate pages currently used functions
          \end{itemize}
    \item \texttt{heap}
          \begin{itemize}
              \item often sparsely-used
              \item in contrast to code: just zero phys.\ page before mapping it
          \end{itemize}
    \item \texttt{stack}: often sparsely-used
\end{itemize}

\paragraph{Page Faults}
The VMA is used to differentiate between \textbf{soft} and \textbf{hard page faults}:
\begin{itemize}
    \item \textbf{Hard page faults} are caused by invalid memory access due to a software bug. Either there is no VMA of permissions are not given. This results in the termination of the application.
    \item \textbf{Soft page faults} occur when valid memory (in VMA and permissions given) is accessed with an invalid page table entry. In this case a new page table entry needs to be created (see~\ref{page demanding} Demand Paging).
\end{itemize}



\paragraph{Demand Paging}\label{page demanding}
In demand paging, memory is only allocated on demand by using \textbf{soft page faults} at the first memory usage.

\newpar{}
\ptitle{Example procedure}

\begin{enumerate}
    \item Application tries to store data
          \texttt{sw t0, 0(t1)}
    \item The MMU walks the pages and gets to an entry with the valid bit set to zero.~\texttt{stvec} is called.
    \item The kernel checks if the requested address (found in \texttt{stval}) is in the valid area (VMA) and if the permissions are given. If not: kill process. If given, continue with 4.
    \item Then the page gets allocated
          \begin{itemize}
              \item permissions according to the VMA
              \item page type (normal, huge etc.) according to the VMA
          \end{itemize}
    \item The store instruction is re-executed at \texttt{sepc}
\end{enumerate}

\newpar{}
\ptitle{Demand Paging All The Way}

This dynamic approach can be used for all the app memory so that we do not have to pre-allocate any page tables.

\paragraph{Page Sharing}
There are 3 common use cases for page sharing.

\newpar{}
\ptitle{Communication}

\begin{itemize}
    \item mapping same physical page to the virtual pages of multiple processes
    \item all processes have r/w permissions or some have r, other w
          \begin{itemize}
              \item intended to modify other's contents for communication
          \end{itemize}
\end{itemize}

\newpar{}
\ptitle{Shared Libraries}

Map virtual addresses of multiple processes to same copy of a library. Execute-only permission for security.

\newpar{}
\ptitle{Copy-On-Write (COW)}

\begin{itemize}
    \item map all \texttt{.bss} sections to the same physical zero-page (read-only)
    \item upon first write access:
          \begin{enumerate}
              \item soft page fault: allocate new page, set it to zero but with write permissions (validity checked via VMA). 
              \item Write with valid access.
          \end{enumerate}
\end{itemize}

\paragraph{Page Swapping}
Idea: move pages to SSD if all RAM is used
\begin{itemize}
    \item unset present bit in the PTE. This triggers a PF on access
    \item store physical address on SSD in the PTE to find page later
\end{itemize}

\newpar{}
\ptitle{Handling the PF}

On a (soft) PF the kernel
\begin{itemize}
    \item consults VMA to check whether it is indeed a soft PF
    \item checks PTE to find that page is indeed swapped out
    \item swaps out another victim page
    \item allocates the new free page, update PTE
\end{itemize}

\newpar{}
\ptitle{Swapping Policy}

\begin{itemize}
    \item various, e.g.\ LRU
    \item support from paging HW: level-0 PTEs have recently-accessed bit
          \begin{itemize}
              \item don't swap these pages out
          \end{itemize}
\end{itemize}

\subsection{Exceptions}\label{exceptions}
An exception in the pipeline can either be generated
\begin{itemize}
    \item \textbf{internally} by
          \begin{itemize}
              \item incorrect behavior like executing an invalid instruction.
              \item a potentially valid behavior like page faults.
          \end{itemize}
    \item \textbf{externally} by
          \begin{itemize}
              \item interrupts like a network packet arrival or a key press.
          \end{itemize}
\end{itemize}

Exceptions are handled by flushing the pipeline and moving control to a pre-configured OS routine.

\begin{center}
    \includegraphics[width=\linewidth]{exception_handling.png}
\end{center}

\newpar{}
\ptitle{STVEC} Supervisor Trap Vector register

On an exception the CPU jumps to the address stored in STVEC

\newpar{}
\ptitle{SEPC} Supervisor Exception Program Counter register

SEPC will hold the PC of the offending instruction (to be used by the routine at STVEC)

\newpar{}
\ptitle{SCAUSE} Supervisor Exception CAUSE register

SCAUSE will tell why the exception happened (e.g., a page fault)

\newpar{}
\ptitle{STVAL} Supervisor Trap Value register

STVAL will provide extended information about the exception (e.g., page fault's address)

\newpar{}
\ptitle{Example}

On a soft PF the CPU jumps to STVEC. The handler routine sees which instruction caused the PF (the instruction at SEPC), knows from SCAUSE that the reason was a PF and finds the (user mode virtual) address (page number) at which the exception occurred in STVAL. A routine can then be called to load the missing code page to the address stored in STVAL.
% TODO: how is this not the exact same as above?