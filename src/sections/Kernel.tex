\section{Kernel}
Privileged software that is in charge of securely multiplexing hardware resources between competing processes.

\subsection{Physical Memory Management}
\begin{itemize}
    \item Memory is usually DRAM (capacitors that need to be refreshed periodically)
    \item Last parallel bus (64/72 Bit) in modern computers
\end{itemize}


\subsubsection{Memory Sections}
\includegraphics[width = \linewidth]{memory_sections.png}

Kernel allocates code/stack at program start. Heap is allocated dynamically by the kernel and managed by the heap allocator library (e.g.\ \textit{heap.so}, part of application).

\ptitle{.text}

\begin{itemize}
    \item Generated by compiler
    \item Code (machine instructions)
    \item Does not change after the program ist written (except browser)
\end{itemize}


\ptitle{.data/.bss}

\begin{itemize}
    \item Generated by compiler
    \item \textit{.data}: initialized global vars, static function vars (sizes known beforehand)
    \item \textit{.bss}: same as above but not initialized: allocate a zero page and point to it (no physical memory usage)
\end{itemize}

\ptitle{Stack}

\begin{itemize}
    \item Compiler manages fixed-sized stack
    \item Local variables, callee-saved registers, return address
\end{itemize}

\ptitle{Heap}
\begin{lstlisting}[language={C}]
    void *ptr = malloc(size_t);
    free(ptr);                  
\end{lstlisting}
\begin{itemize}
    \item User steers lifetime (new, delete)
    \item Available across functions
    \item Used also for large allocations
\end{itemize}

\subsubsection{Memory Allocators in Jake}
\begin{center}
    \includegraphics[width=\linewidth]{Allocators.png}
\end{center}
% TODO: we cuold remove this, since it's already shown in the diagramm
\begin{itemize}
    \item Slab allocator built \textit{on top} of low-level allocator:
          \begin{itemize}
            %   \item Kernel (buddy allocator) provides memory and can grow full slabs / shrink empty ones
              \item Slab allocator for small memory requests within pages
          \end{itemize}
    \item Applications' allocators ask kernel (buddy) for memory 
\end{itemize}

\subsection{Dynamic Memory Management (Heap)}
\begin{itemize}
    \item Heap suffers \textbf{fragmentation} at runtime: causes non-contiguous memory
          \noindent\begin{itemize}
              \item \textbf{External fragmentation} is caused by disadvantageous sequences of allocations/freeings (there would be enough free heap but it is not contiguous)
              \item \textbf{Internal fragmentation} is caused by having large heap units where small allocations waste memory
          \end{itemize}
    \item \textbf{Heap metadata} are used to keep track of free memory and for sanity checks
          \noindent\begin{itemize}
              \item \textbf{In-band heap metadata} are stored inside the heap
              \item \textbf{Out-of-band heap metadata} are kept outside the heap
          \end{itemize}
\end{itemize}

\subsubsection{Linked List (LL): In-Band Metadata}
\begin{itemize}
    \item [+] no internal fragmentation
    \item [-] external fragmentation
\end{itemize}

\newpar{}
\ptitle{Handling Fragmentation}

\begin{itemize}
    \item Coalesce/combine LL (required in contrast to slab allocation): walk the list, find adjacent free items and merge them.
\end{itemize}

\newpar{}
\ptitle{Data Structure}
\begin{lstlisting}[language={C}]
typedef struct __node_t {
    int size;                           // allocation size
    union {                             // contains either:
        int magic;                      // sanity check or
        struct __node_t *next;          // next free section
    } u;
} node_t;         
\end{lstlisting}

\includegraphics[width = \linewidth]{free_linked_list.png}

\begin{itemize}
    \item Nodes (8 B) stored at start of each free heap section contain information about size and status
    \item Use a LL to keep track of \textbf{free} heap sections
\end{itemize}

\newpar{}
\ptitle{malloc}
\begin{enumerate}
    \item Walk the linked list to find free space
    \item Set magic value and remove node from free LL (update pointers)
    \item Return pointer to allocated heap
\end{enumerate}

\newpar{}
\begin{itemize}
    \item \textbf{First fit}: take first free slot that is big enough (counterintuitively it is more space efficient on the long run)
    \item \textbf{Best fit}: take slot matching the required size the best (slow: walk entire list. Small losses on each allocation: less space efficient on the long run)
\end{itemize}

\newpar{}
\ptitle{free}

\begin{enumerate}
    \item Check magic value. If it has been overwritten: out-of-bound software bug
    \item Allocate a new free node
    \item Insert node to the front of the free LL:
          \begin{itemize}
              \item \fncode{*next = head}
              \item \fncode{head = node}
          \end{itemize}
\end{enumerate}
A \textit{doubly linked list} can be used for convenient unlinking

\subsubsection[Bitmap-Based Allocations]{Bitmap-Based Allocations: Out-Of-Band Metadata}
Every bit can tell whether $N$ bytes of the heap are free
\begin{itemize}
    \item [+] Less external fragmentation than in LL
    \item [+] Coalescing for free
    \item [-] More internal fragmentation: inflexible block size
    \item [-] \textbf{Additional} data to keep track of total allocation sizes (as we only know status of our $N$ byte units)
\end{itemize}

\newpar{}
\ptitle{malloc}

Scans the bitmap for free space %ChkTex 36
\begin{itemize}
    \item Update allocation size in additional metadata
\end{itemize}

\newpar{}
\ptitle{free}

Flips the right bits in the bitmap to zero %ChkTex 36
\begin{itemize}
    \item Get size to free from additional metadata
\end{itemize}

\newpar{}
\ptitle{Bitmap Granularity}

\begin{itemize}
    \item Depending on the bitmaps granularity, there is a tradeoff between \textbf{fragmentation} and \textbf{performance}:%ChkTex 36
          \begin{itemize}
              \item fine bitmap: low internal fragmentation but expensive \code{malloc()} %ChkTex 36
              % TODO: Is it expensive because of more external fragmentation or because more "small steps" have to be taken on malloc?
              \item coarse bitmap: high internal frag., cheap \code{malloc()} %ChkTex 36
          \end{itemize}
\end{itemize}


\subsubsection{Slab Allocator: In-/Out-Of-Band Metadata}
Create ``sub-heaps'' (slabs) for common allocation sizes to reduce ext./int.\ fragmentation. Each slab contains e.g. 12/16$\dots$ 256-byte entries.

\begin{itemize}
    \item[+] Flexible (various allocation sizes, growing/shrinking)
    \item[-] Some internal fragmentation
    \item[-] External fragmentation: \textit{Fragmentation across slabs} (not optimized for full utilization): Can't move memory between slabs as they become full/empty.\ \textbf{Buddy} solves this issue.
\end{itemize}

\newpar{}
\ptitle{Implementation}
% so much duplication :(((
\begin{itemize}
    \item LL:
          \begin{itemize}
              \item [+] No coalescing due to \textit{equal, fixed sizes} per slab (in contrast to vanilla LL allocator)
              \item [+] Fast malloc: no scanning (just take 1st element of free list)
              \item [-] Extra metadata
          \end{itemize}
    \item Bitmap:
          \begin{itemize}
              \item [+] Coalescing for free
              \item [+] Less metadata: single bit
              \item [-] Slow malloc due to scanning
          \end{itemize}
\end{itemize}

\subsubsection{Page Allocation: Buddy Allocator}
Page allocation optimizes for \textbf{full utilization}.
\begin{itemize}
    \item A page is a \textbf{contiguous area of memory} aligned to the page size (4 KB)
    \item Blocks can have a size that scales with power-of-two wrt.\ the page size, this power is also called the \code{order}.
          \begin{itemize}
              \item Code, stack, heap are at least 1 page each.
          \end{itemize}
    \item Each page has a \textit{buddy} with the same size.
          \begin{itemize}
              \item If a page is split, a new page and a buddy with \code{order - 1} are generated.
              \item If a page is freed and its buddy is also free, these two pages are merged into a page with \code{order + 1}.
          \end{itemize}
    \item Page allocation finds tradeoff between performance and efficient memory usage
\end{itemize}

\paragraph{Buddy Allocator: Interface}
\ptitle{\code{block\_alloc(order)}} %ChkTex 36
\begin{enumerate}
    \item if no block of \code{order} is available, split higher order block recursively.
    \item allocate block of desired \code{order}
\end{enumerate}

Implementation:
\begin{lstlisting}[style=bright_C++]
struct block *buddy_alloc(unsigned order)
{
    if (order > BUDDY_MAX_ORDER) return NULL;
    int smallest_order = __buddy_find_smallest_free_order(order);
    if (smallest_order == -1) return -1;

    // does nothing if "smallest_order == order"
    // else split "smallest_order" until "order"
    int ret = buddy_split(smallest_order, order);
    if(ret == -2) return NULL;

    return buddy_pop(order);
}
\end{lstlisting}

\newpar{}
\ptitle{\code{block\_free(block)}} %ChkTex 36
\begin{enumerate}
    \item free block
    \item if buddy is free, merge into larger block
\end{enumerate}

Implementation:
\begin{lstlisting}[style=bright_C++]
int buddy_free(struct block *block)
{
    switch (block->refcnt) {
    case 0:
        return -1; /* Double free */
    case 1:
        buddy_push(block, block->order);
        __buddy_try_merge(block);

        return 0;
    default:
        block->refcnt--;
        return 0;
    }
}
\end{lstlisting}

\textbf{Remarks}
\begin{itemize}
    \item Both \code{block\_alloc} and \code{block\_free} trigger a max of (\code{order}) of operations. %ChkTeX 36 
    \item Initialization: Call \code{block\_free} on every 4kB page (causes merging).
\end{itemize}

\paragraph{Buddy Allocator: Data Structures}
\ptitle{Block} (Bitmap)
\begin{lstlisting}[style=bright_C++]
struct block {
    unsigned refcnt;    /* e.g. for shared memory */
    unsigned order; /* 0 <= order <= BUDDY_MAX_ORDER */
    struct block *next;
};
\end{lstlisting}
\newpar{}
\begin{lstlisting}[style=bright_C++]
static struct block buddy_blocks[1UL << BUDDY_MAX_ORDER];
\end{lstlisting}

\ptitle{Free List}
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_free_lists[BUDDY_MAX_ORDER + 1];
\end{lstlisting}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item Static heap size assumed in Jake: \texttt{buddy\_blocks} in \texttt{.bss}
    \item Static assumption not used in general OS (variable heap size per system)
          \begin{itemize}
              \item Use bootmem allocator
          \end{itemize}
    \item \texttt{buddy\_free\_lists} to avoid bitmap scans: cheap malloc
\end{itemize}

\paragraph{Buddy Allocator: Internal Functions}
\ptitle{Block to Buddy}
\begin{lstlisting}[style=bright_C++]
static inline struct block *block2buddy(struct block *block, int order)
{
    return __ppn2block(block2__ppn(block) ^ 1UL << order);
}
\end{lstlisting}
Note: If address instead of page number is used: flip \texttt{12+order+1}th bit.

\newpar{}
\ptitle{Smallest Order}
\begin{lstlisting}[style=bright_C++]
static int __buddy_find_smallest_free_order(unsigned order)
{
    for (int i = order; i <= BUDDY_MAX_ORDER; i++) {
        if (!is_list_empty(i)) return i;
    }
    return -1;
}
\end{lstlisting}

\paragraph{Buddy Allocator: Managing the Free LL}
\ptitle{Push}

Push to the front of \code{buddy\_free\_lists}
\begin{lstlisting}[style=bright_C++]
static void buddy_push(struct block *block, unsigned order)
{
	block->next = buddy_free_lists[order];
	buddy_free_lists[order] = block;
	block->refcnt--;
}
\end{lstlisting}

\newpar{}
\ptitle{Pop}

Pop from the front of \code{buddy\_free\_lists}
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_pop(unsigned order)
{
    if (is_list_empty(order)) return NULL;

    struct block *block = buddy_free_lists[order];

    /* update list and pop front of linked list */
    if (block->next != NULL) {
        buddy_free_lists[order] = block->next;
        block->next = NULL;
    } else {
        buddy_free_lists[order] = NULL;
    }
    block->refcnt++;

    return block;
}
\end{lstlisting}

\newpar{}
\ptitle{Remove}

Find \code{block} in  \code{buddy\_free\_lists}, move to front and \code{pop}.
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_remove(struct block *block, unsigned order)
{
	if (is_list_empty(order)) return NULL;

	/* Find block in linked list and bring it to the front */
	if (buddy_free_lists[order] != block) {
		struct block *head = buddy_free_lists[order];
		struct block *tail = head;
		struct block *prev = NULL;

		while (tail->next) {
			if (tail->next == block) prev = tail;
			tail = tail->next;
		}

		if (!prev) return NULL; /* Could not find block */

		buddy_free_lists[order] = block;
		prev->next = NULL;
		tail->next = head;
	}

	return buddy_pop(order);
}
\end{lstlisting}

\paragraph{Buddy Allocator: Moving Memory Accross Levels}
\ptitle{Split}

Splits block \textbf{iteratively}.
\begin{lstlisting}[style=bright_C++]
static int buddy_split(unsigned smallest_free_order, unsigned desired_order)
{
	/* Will do nothing if smallest_free and desired are equal */
	for (int i = smallest_free_order; i > desired_order; i--) {
		int ret = __buddy_split(i);
		if (ret) return ret;
	}
	return 0;
}
\end{lstlisting}

\newpar{}
\begin{lstlisting}[style=bright_C++]
static int __buddy_split(unsigned order)
{
    if (order == 0) return -1;

    struct block *block = buddy_pop(order);

    if (!block) return -2; // no block available

    /* create buddy */
    block->order--;
    struct block *buddy = block2buddy(block, block->order);
    buddy->order = block->order;

    /* push split blocks to free lists */
    buddy_push(block, block->order);
    buddy_push(buddy, buddy->order);

    return 0;
}
\end{lstlisting}

\newpar{}
\ptitle{Merge}

Merges block and buddy \textbf{recursively}.
\begin{lstlisting}[style=bright_C++]
static void __buddy_try_merge(struct block *block)
{
    if (block->order == BUDDY_MAX_ORDER) return;

    /* Get buddy of block */
    struct block *buddy = block2buddy(block, block->order);

    if (block->order != buddy->order) return;

    if (is_block_free(block) && is_block_free(buddy)) {
        block = __buddy_merge(block, buddy);
        __buddy_try_merge(block);
    }
}
\end{lstlisting}
\newpar{}
\begin{lstlisting}[style=bright_C++]
static struct block *__buddy_merge(struct block *block, struct block *buddy)
{
    /* Remove them from current order's list */
    buddy_remove(buddy, buddy->order);
    buddy_remove(block, block->order);

    /* Merge into the one with the smaller address */
    if (block2__ppn(block) < block2__ppn(buddy)) {
        block->order++;
        buddy_push(block, block->order);
        return block;
    } else {
        buddy->order++;
        buddy_push(buddy, buddy->order);
        return buddy;
    }
}
\end{lstlisting}

\subsection{Virtual Memory}
% Segmentation and x86 history left out on purpose
With virtual memory, the users contiguous memory is spread out in physical memory to improve flexibility and external fragmentation.
In order to achieve this, the memory management unit (\textbf{MMU}) translates memory addresses accordingly to create the illusion of contiguous memory for the user. In addition the MMU also controls the permissions to that virtual memory space.

\subsubsection{Memory Access in Jake}
\begin{center}
    \includegraphics[width=0.9\linewidth]{memory_access.png}
\end{center}

\subsubsection{Segmentation}

\begin{itemize}
    \item Inflexible
    \item Suffers from both internal and external fragmentation
\end{itemize}

\subsubsection{Paging}
Solves issues arising from segmentation.

\begin{center}
    \includegraphics[width = \linewidth]{kernel_virt_mem.png}
\end{center}

\ptitle{Page Table}
\begin{itemize}
    \item translates the virtual page into a physical one
    \item In RISC-V, Pages have 4 KB (12 bit) granularity
          \begin{itemize}
              \item Reason for buddy page size
          \end{itemize}
    \item Kernel allocates pages (buddy)
    \item Page table entries are located in the kernel vma (stack/heap)
    \item CPU's MMU translates addresses using the page table
\end{itemize}

\paragraph{Linear Page Table}
\begin{itemize}
    \item simple
    \item large unused parts (virtual $>>$ physical)
          \begin{itemize}
              \item wastes physical memory
          \end{itemize}
    \item the virtual address is the row index of the page table
          \begin{itemize}
              \item virtual offset = physical offset within page
          \end{itemize}
\end{itemize}

\ptitle{Dimensions}
\noindent\begin{equation*}
    s_{\text{page table}} = n_{\text{virt}}\cdot (\log_2(n_{\text{phys}}) + 1)\quad [\text{bits}]
\end{equation*}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}
\begin{tabularx}{\linewidth}{@{} c c c@{}}
                                                                                   & physical page                  & present bit \\
    \cmidrule{2-3}
    \multirow{7}{*}{\begin{sideways}$n_{\text{virt}}$\end{sideways}} & 10                             & 1           \\
                                                                                   & 01                             & 1           \\
                                                                                   & x                              & 0           \\
                                                                                   & x                              & 0           \\
                                                                                   & 00                             & 1           \\
                                                                                   & 11                             & 1           \\
                                                                                   & x                              & 0           \\
                                                                                   & x                              & 0           \\
    \cmidrule{2-3}
                                                                                   & $\log_2(n_{\text{phys}})$ bits &
\end{tabularx}

\paragraph{Multi-level Page Tables}
\begin{itemize}
    \item Pages are allocated when needed
    \item Stored im DRAM
    \item In our case (\textbf{Sv48}):
          \begin{itemize}
              \item 48 bit virtual and 56 bit physical address space
              \item 4 levels
              \item page table entry (PTE) is 8 bytes (64 bit)
              \item page is 4 KB $\to \frac{\text{4 KB}}{\text{8 B / entry}} = 512$ entries
                    \begin{itemize}
                        \item $\log_{2}(512) = 9$ bits required to index page table to get a PTE
                    \end{itemize}
              \item virtual page number: 4 (levels) $\cdot$ 9 bits = 36 bits
                    \begin{itemize}
                        \item the remaining $48-36=12$ bits are used to index the ${2}^{12} = 4$ KB of physical bytes
                    \end{itemize}
          \end{itemize}
\end{itemize}

\begin{center}
    \includegraphics[width =\linewidth]{kernel_sv48.png}
\end{center}

\ptitle{Virtual $\to$ physical (memory walk)}
\begin{itemize}
    \item Done by CPU's MMU
\end{itemize}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_ml_mem_walk.png}
\end{center}

\newpar{}
\ptitle{Creation}
\begin{itemize}
    \item Done in software (by OS)
    \item Identical to memory walk.
    \item If \code{present == 0}, allocate new page (\code{buddy\_alloc(0)} $\to$ \code{ppn}) and update PPN.\ %ChkTeX 36
\end{itemize}

\newpar{}
\ptitle{Page Table Entry}

\begin{center}
    \includegraphics[width = \linewidth]{kernel_page_entry.png}
\end{center}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}llcll@{}}
    V & valid     &  & G    & global (vma)  \\
    R & read      &  & A    & accessed      \\
    W & write     &  & D    & dirty (write) \\
    X & execute   &  & PBMT & IO            \\
    U & user mode &  & N    & TLBOPT
\end{tabularx}

\begin{tabularx}{\linewidth}{@{}cccl@{}}
    X & W & R & Meaning                             \\
    \cmidrule{1-4}
    0 & 0 & 0 & Pointer to next level of page table \\
    0 & 0 & 1 & Read-only page                      \\
    0 & 1 & 0 & Reserved for future use             \\
    0 & 1 & 1 & Read-write page                     \\
    1 & 0 & 0 & Execute-only page                   \\
    1 & 0 & 1 & Read-execute page                   \\
    1 & 1 & 0 & Reserved for future use             \\
    1 & 1 & 1 & Read-write-execute page
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\begin{itemize}
    \item \texttt{V} Stores whether the memory has already been mapped.
    \item \texttt{G} pages are not flushed by TLB as they are globally valid.
    \item \texttt{A} indicates accesses (e.g.\ useful for page swapping).
    \item \texttt{D} indicates write accesses (e.g.\ useful for TLB to keep TLB/DRAM synchronized).
    \item \texttt{U} kernel/user permission, can be compared to the \texttt{sstatus} which stores the requesting mode.
\end{itemize}

\newpar{}
\ptitle{satp}
\begin{itemize}
    \item \code{satp} (supervisor address translation and protection) register points to the root of the page table
    \item \code{mode} turns on and specifies the MMU type
\end{itemize}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_satp.png}
\end{center}
\begin{center}
    \includegraphics[width = .6\linewidth]{kernel_satp_table.png}
\end{center}

\newpar{}
\ptitle{Bigger Page Types}

Every memory access requires 4 additional lookups (memory accesses). If the applications only need bigger junks of memory then the lookup tree can be pruned by a page (or more).

\newpar{}
One page table has 512 entries ($2^9$).
\begin{align*}
    \text{page}       & = & 4\text{~KB}         &   &               \\
    \text{Huge page}  & = & 512 * 4\text{~KB}   & = & 2\text{~MB}   \\
    \text{Super page} & = & 512^2 * 4\text{~KB} & = & 1\text{~GB}   \\
    \text{Giant page} & = & 512^3 * 4\text{~KB} & = & 512\text{~GB}
\end{align*}

\begin{itemize}
    \item To determine if the current page table entry points to a next page table or to a bigger page size the X, W, R flags in are used (all 0 = next level of page table).
    \item In case of e.g.\ a huge page the permission bits are already set at level 1 instead of level 0.
\end{itemize}

% TODO: I don't get what this example shows us. For me this is just a specific implementation and not a general concept.
\subsubsection{Page Setup in Jake}

\newpar{}
\ptitle{Utils}
\begin{lstlisting}[style=bright_C++]
static void pt_walker_set_nonleaf_pte(pte_t *pte){
    pte_t table = pt_alloc_page_table();

    assert(pte_is_valid(table));

    *pte = table;
    pt_flush_tlb();
}
\end{lstlisting}

\begin{lstlisting}[style=bright_C++]
static pte_t *pt_walker_next_pte(pte_t *pte_to_start_from,
                unsigned long depth_of_pte_to_start_from,
                unsigned long vpn)
{
    unsigned long phys_addr_of_next_pte = 0;
    pte_t *next_pte = NULL;
    phys_addr_of_next_pte = 
        ((ppn2phys(pte_get_ppn(*pte_to_start_from))) + 
        ((vpn2page_table_index_at_level(vpn, 
        depth2level(depth_of_pte_to_start_from+1)))<<3));
    next_pte = phys2virt(phys_addr_of_next_pte);

    return next_pte;
}
\end{lstlisting}

\newpar{}
\ptitle{Set PTE}
\begin{lstlisting}[style=bright_C++]
int pt_walker_set_pte(unsigned long satp, 
    unsigned long vpn, pte_t pte_val,
    unsigned depth){
    // preps
    /* NOTE: pointer to curr. PTE, not to where curr. PTE points to! */
    pte_t root = satp2pte(satp);
    pte_t *pte = &root;

    // checks
    assert(pte_is_valid(pte_val));
    assert(depth > 0);
    assert(depth < PT_NUM_LEVELS);

    // the actual PT walk
    for(int d = 0; d<depth; d++){
        // next line always ok at satp
        pte = pt_walker_next_pte(pte, d, vpn);		
        if (pte_is_leaf(*pte)) 
            return -EMAPPED; // mapped :(==
        if(!pte_is_valid(*pte)) 
            pt_walker_set_nonleaf_pte(pte); // allocate
    }

    // being here implies: d=depth, valid, nonleaf
    *pte = pte_val;     // set the pte
    pt_flush_tlb();
    return 0;
}
\end{lstlisting}

\subsubsection{Virtual Memory Management}
The virtual memory space is divided into two sections one for the application and one for the kernel.
\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_app_kernel_space.png}
\end{center}

The kernel memory space contains five regions:

\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_kernel_space.png}
\end{center}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}lXccc@{}}
    Section        & Description                         & X & W & R \\
    \midrule
    \texttt{I/O}   & Interaction with devices            & 0 & 1 & 1 \\
    \texttt{.text} & Kernel code                         & 1 & 0 & 0 \\
    \texttt{.data} & Initialized global/static variables & 0 & 1 & 1 \\
    \texttt{ID}    & Identity map of physical memory     & 0 & 1 & 1 \\
    \texttt{stack} & Kernel stack                        & 0 & 1 & 1
\end{tabularx}

\newpar{}
\ptitle{Remark}

\texttt{ID}
\begin{itemize}
    \item maps the entire physical memory left
    \item is required for manipulating page tables
          % TODO: Additional purposes?
\end{itemize}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\paragraph{Booting Jake}
\begin{enumerate}
    \item Bootloader loads kernel binary from SSD to beginning of DRAM
    \item Kernel assigns some physical memory as kernel stack
          % TODO: Creates buddy datastructures on kernel stack?
    \item Enable virtual memory (turn the MMU on)
\end{enumerate}

\paragraph{MMU Start-Up}

For backward compatibility reasons the kernel is located in the upper part of the virtual memory. This introduces a problem with the PC when the MMU starts. First the PC is pointing to the physical address of the kernel code. But after the MMU initialized the virtual memory space the PC points to the wrong address.

\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_mmu_problem.png}
\end{center}

This is tackled by
\begin{enumerate}
    \item Double mapping the kernel before the MMU starts (create PTEs).
    \item Update pointers (add virtual base)
          \begin{itemize}
              \item Buddy allocator's heap metadata
              \item \code{sp, ra} etc.\
          \end{itemize}
          and JALR to high address space
    \item Change the \fncode{mode} bit in \fncode{satp}  to turn on the MMU
    \item Remove the double mapped kernel.
\end{enumerate}

\subsubsection{VMAs}
% If an application violates the permissions or tries to access an unmapped memory area the CPU throws an exception. (For more details on exceptions see Section~\ref{exceptions}) This exception is called a page fault.
\ptitle{Data Structure}
\begin{lstlisting}[style=bright_C++]
struct vma{
    uintptr_t   vpn;        // start address
    size_t      pagen;
    unsigned    flags;      // permissions
    struct node node;
}
\end{lstlisting}
% TODO: The all the information that was here is already in the sections below and above (U bit)

The kernel has a list of \textbf{virtual memory areas} (VMAs) for each process (and itself). VMAs are managed by the MMU (\code{mmap, munmap}).

% TODO: These are benefits of demand paging not VMA and are already mentioned.
% \newpar{}
% \ptitle{Benefits for Different Memory Regions}
% \begin{itemize}
%     \item \texttt{.code}
%           \begin{itemize}
%               \item apps have many functions: not all used at the same time.
%               \item only allocate pages for currently used functions
%           \end{itemize}
%     \item \texttt{heap}
%           \begin{itemize}
%               \item often sparsely-used
%               \item in contrast to code: just zero physical page before mapping it
%           \end{itemize}
%     \item \texttt{stack}: often sparsely-used
% \end{itemize}

\paragraph{Page Faults}
The VMA is used to differentiate between \textbf{soft} and \textbf{hard page faults}:
\begin{itemize}
    \item \textbf{Hard page faults} are caused by invalid memory access due to a software bug. Either there is no VMA or permissions are not given. This results in the termination of the application.
    \item \textbf{Soft page faults} occur when valid memory (in VMA and permissions given) is accessed with an invalid page table entry. In this case a new page table entry needs to be created (see~\ref{page demanding} Demand Paging).
\end{itemize}



\paragraph{Demand Paging}\label{page demanding}
In demand paging, memory is only allocated on demand by using \textbf{soft page faults} at the first memory usage.

\newpar{}
\ptitle{Example procedure}

\begin{enumerate}
    \item Application tries to store data
          \texttt{sw t0, 0(t1)}
    \item The MMU walks the pages and gets to an entry with the valid bit set to zero.~\texttt{stvec} is called.
    \item The kernel pushes the user registers (TF) to the kernel stack.
    \item The kernel checks if the requested address (found in \texttt{stval}) is in the valid area (VMA) and if the permissions are given. If not: kill process. If given, continue with 5.
    \item Then the page gets allocated
          \begin{itemize}
              \item permissions according to the VMA
              \item page type (normal, huge etc.) according to the VMA
              \item kernel knows from \texttt{scause} that the reason was a PF and finds the (user mode virtual) address (page number) at which the exception occurred in \texttt{stval}.
          \end{itemize}
    \item Exception handler restores TF, then calls \texttt{sret}
          \begin{itemize}
              \item Privilege mode to switch back to is given by \texttt{sstatus.SPP}
              \item Next pc given by \texttt{sepc} (\texttt{sw} is re-executed there)
          \end{itemize}
\end{enumerate}

\newpar{}
\ptitle{Demand Paging All The Way}

This dynamic approach can be used for all the app memory so that we do not have to pre-allocate any page tables.

\paragraph{Page Sharing}
There are 3 common use cases for page sharing.

\newpar{}
\ptitle{Communication}

\begin{itemize}
    \item mapping same physical page to the virtual pages of multiple processes
    \item all processes have r/w permissions or some have r, other w
          \begin{itemize}
              \item modifying other's contents is intended
          \end{itemize}
\end{itemize}

\newpar{}
\ptitle{Shared Libraries}

Map virtual addresses of multiple processes to same copy of a library. Execute-only permission for security.

\newpar{}
\ptitle{Copy-On-Write (COW)}

\begin{itemize}
    \item map all \texttt{.bss} sections to the same physical zero-page (read-only)
    \item upon first write access:
          \begin{enumerate}
              \item soft page fault: allocate new page, set it to zero but with write permissions (validity checked via VMA).
              \item Repeat write with valid access.
          \end{enumerate}
\end{itemize}

\paragraph{Page Swapping}
Idea: Move pages to SSD if all DRAM is used % TODO: This sentence is almost all that is needed for this section. :)
\begin{itemize}
    \item Unset present bit in the PTE. This triggers a PF on access
    \item Store physical address on SSD in the PTE to find page later
\end{itemize}

\newpar{}
\ptitle{Handling the PF}

On a (soft) PF the kernel
\begin{itemize}
    \item consults VMA to check whether it is indeed a soft PF
    \item checks PTE to find that page is indeed swapped out
    \item swaps out another victim page (use \texttt{A} bit)
    \item allocates the new free page, update PTE
\end{itemize}

\newpar{}
\ptitle{Swapping Policy}

\begin{itemize}
    \item various, e.g.\ LRU
    \item support from paging HW: level-0 PTEs have \texttt{A} bit
          \begin{itemize}
              \item don't swap these pages out
          \end{itemize}
\end{itemize}

\subsection{Exceptions}\label{exceptions}
An exception in the pipeline can either be generated
\begin{itemize}
    \item \textbf{internally} by
          \begin{itemize}
              \item incorrect behavior like executing an invalid instruction.
              \item a potentially valid behavior like page faults.
          \end{itemize}
    \item \textbf{externally} by
          \begin{itemize}
              \item interrupts like a network packet arrival or a key press.
          \end{itemize}
\end{itemize}

Exceptions are handled by flushing the pipeline and moving control to a pre-configured OS routine.

\begin{center}
    \includegraphics[width=\linewidth]{exception_handling.png}
\end{center}

\newpar{}
\ptitle{STVEC} Supervisor Trap Vector register

On an exception the CPU jumps to the address stored in STVEC

\newpar{}
\ptitle{SEPC} Supervisor Exception Program Counter register

SEPC will hold the PC of the offending instruction (to be used by the routine at STVEC)

\newpar{}
\ptitle{SCAUSE} Supervisor Exception CAUSE register

SCAUSE will tell why the exception happened (e.g., a page fault)

\newpar{}
\ptitle{STVAL} Supervisor Trap Value register

STVAL will provide extended information about the exception (e.g., page fault's address)

\subsubsection{Storing the Trap Frame (TF)}

User mode stack is a bad choice for storing the TF:
\begin{itemize}
    \item The stack might not be mapped.
    \item Supervisor might not have access to user mode pages.
    \item Kernel must zero registers after using user-mode stack to not disclose sensitive information (slow).
\end{itemize}
Therefore, the kernel stack is commonly used. Its location is stored in the \texttt{sscratch} registers.

\newpar{}
\ptitle{Excerpt of Trap Handler}

\begin{lstlisting}[language={[RISC-V]Assembler}]
trap_handler_s:
	csrrw sp, sscratch, sp  // sscratch = sp_u  
    addi sp, sp, -264
    
	sd ra, 8(sp)
	sd t0, 16(sp)
	sd t1, 24(sp)
	sd t2, 32(sp)
    .
    .
\end{lstlisting}