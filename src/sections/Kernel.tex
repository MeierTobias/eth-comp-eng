\section{Kernel}
Privileged software that is in charge of securely multiplexing hardware resources between competing processes.

\subsection{Memory Management}
\begin{itemize}
    \item Memory is usually DRAM (capacitors that need to be refreshed periodically)
    \item Last parallel bus (64/72 Bit) in modern computers
\end{itemize}


\subsubsection{Memory Sections}
\includegraphics[width = \linewidth]{memory_sections.png}

Kernel allocates code/stack at program start. Heap is allocated dynamically by the kernel and managed by the heap allocator library (e.g.\ \textit{heap.so}, part of application).

\ptitle{.text}

\begin{itemize}
    \item Generated by compiler
    \item Code (machine instructions)
    \item Does not change after the program ist written (except browser)
\end{itemize}


\ptitle{.data/.bss}

\begin{itemize}
    \item Generated by compiler
    \item \textit{.data}: initialized global vars, static function vars (sizes known beforehand)
    \item \textit{.bss}: same as above but not initialized: allocate a zero page and point to it (no physical memory usage)
\end{itemize}

\ptitle{Stack}

\begin{itemize}
    \item Compiler manages fixed-sized stack
    \item Local variables, callee-saved registers, return address
\end{itemize}

\ptitle{Heap}
\begin{lstlisting}[language={C}]
    void *ptr = malloc(size_t);
    free(ptr);                  
\end{lstlisting}
\begin{itemize}
    \item User steers lifetime (new, delete)
    \item Available across functions
    \item Used also for large allocations
\end{itemize}

\subsubsection{Managing the Heap}
\begin{itemize}
    \item Heap suffers \textbf{fragmentation} at runtime: causes non-contiguous memory
          \noindent\begin{itemize}
              \item \textbf{External fragmentation} is caused by disadvantageous sequences of allocations/freeings (there would be enough free heap but it is not contiguous)
              \item \textbf{Internal fragmentation} is caused by having large heap units where small allocations waste memory
          \end{itemize}
    \item Fragmentation and finding free memory must be handled
    \item \textbf{Heap metadata} are used to keep track of free memory and for sanity checks
          \noindent\begin{itemize}
              \item \textbf{In-band heap metadata} are stored inside the heap
              \item \textbf{Out-of-band heap metadata} are kept outside the heap
          \end{itemize}
\end{itemize}

\paragraph{In-Band Metadata: Linked List (LL)}
\includegraphics[width = \linewidth]{free_linked_list.png}

\begin{itemize}
    \item Use a LL to keep track of \textbf{free} heap sections
    \item Nodes stored at start of each free heap section
    \item External fragmentation but no internal fragmentation
\end{itemize}

\begin{lstlisting}[language={C}]
typedef struct __node_t {
    int size;                           // allocation size
    union {
        int magic;                      // sanity check
        struct __node_t *next;          // next free section
    } u;
} node_t;         
\end{lstlisting}
\textbf{Remark}

Union represents either \code{magic} value \textbf{or} \code{*next} node variable.
\begin{itemize}
    \item \code{magic} value for allocated areas.
    \item \code{*next} node for free elements.
\end{itemize}

\ptitle{Malloc}

\begin{itemize}
    \item Walk the linked list at \lstinline{malloc()} %ChkTex 36
          \begin{itemize}
              \item \textbf{First fit}: take first free slot that is big enough
              \item \textbf{Best fit}: take slot matching the required size the best (slow: walk entire list)
          \end{itemize}
    \item First fit uses space more efficiently than best fit (small losses at each allocation!)
    \item As slot is selected: set magic value / remove corresponding node from list, update pointers, return pointer to free heap
\end{itemize}

\ptitle{Free}

\begin{itemize}
    \item \lstinline{free(a)}%ChkTex 36
    \item Allocate a new \lstinline{__node_t} data structure
    \item Check magic value. If it has been overwritten: out-of-bound software bug
    \item Let \lstinline{ __node_t *next} point to previous \lstinline{head}
    \item Make the new node the new \lstinline{head}
    \item A \textit{doubly linked list} can be used for convenient unlinking
\end{itemize}

\ptitle{Handling Fragmentation}

\begin{itemize}
    \item Coalesce/ combine the free list
    \item Walk the list, find adjacent free items and merge them
\end{itemize}

\paragraph[Bitmap-Based Allocations]{Out-Of-Band Metadata:\newline Bitmap-Based Allocations}
\begin{itemize}
    \item Every bit can tell whether $N$ bytes of the heap are free
    \begin{itemize}
        \item Can be more wasteful than LL metadata: inflexible block size
        \item Internal fragmentation    % TODO: In the book it says that variable-size allocations in bitmap allocators cause internal fragmentation. But isn't there also external fragmentation in bitmap allocators?
    \end{itemize}
    \item One needs \textbf{additional} data to keep track of the total allocation sizes (as we only know status of our $N$ byte units)
    \item \code{malloc()} scans the bitmap for free space %ChkTex 36
    \begin{itemize}
        \item Update allocation size in additional metadata
    \end{itemize}
    \item \code{free()} flips the right bits in the bitmap to zero %ChkTex 36
    \begin{itemize}
        \item Get size to free from additional metadata
        \item Coalescing comes for free
    \end{itemize}
\end{itemize}

\ptitle{Fragmentation vs. Performance}

\begin{itemize}
    \item Coalescing is very cheap (flipping bits)
    \item \code{malloc()} is more expensive (scan the bitmap), depending on how coarse the bitmap is %ChkTex 36
    \item In a \textbf{fine bitmap} one has precise allocations (low internal fragmentation) but expensive \code{malloc()} %ChkTex 36
    \item In a \textbf{coarse bitmap} one has more bytes per bit, hence less precise allocations (high internal frag) but cheap \lstinline{malloc()} %ChkTex 36
\end{itemize}


\paragraph{In-/Out-Of-Band Megadata: Slab Allocation}

\begin{itemize}
    \item Create ``sub-heaps'' (slabs) for common allocation sizes to reduce ext./int.\ fragmentation
    \item Each slab contains e.g. 12/16/256-byte entries
    \item Applications' allocators ask kernel for memory
    \item Built \textit{on top} of low-level allocator: Kernel provides memory and can grow full slabs / shrink empty ones
    \item Implementation:
    \begin{itemize}
        \item LL: No coalescing, fast malloc but extra metadata
        \item Bitmap: Single bit metadata but slower malloc due to scanning
    \end{itemize}
\end{itemize}

\ptitle{Pros and Cons}
\begin{itemize}
    \item[+] Flexible (various allocation sizes, growing/shrinking)
    \item[-] Some internal fragmentation
    \item[-] External fragmentation: \textit{Fragmentation across slabs} (not optimized for full utilization): Can't move memory between slabs as they become full/empty
    \begin{itemize}
        \item Solution: Use page allocation to move pages between different slabs
    \end{itemize}
\end{itemize}
\paragraph{Page Allocation: Buddy Allocator}
Page Allocation optimizes for \textbf{full utilization}.
\begin{itemize}
    \item A page is a \textbf{contiguous area of memory} aligned to the page size (4 KB)
    \item Blocks can have a size that scales with power-of-two wrt.\ the page size, this power is also called the \textbf{order}.
    \begin{itemize}
        \item Code, stack, heap are at least 1 page each.
    \end{itemize}
    \item Each page has a \textit{buddy} with the same size.
          \begin{itemize}
              \item If a page is split, a new page and a buddy with order - 1 are generated.
              \item If a page is freed and its buddy is also free, these two pages are merged into a page with order + 1.
          \end{itemize}
    \item Page allocation finds tradeoff between performance and efficient memory usage
\end{itemize}

\paragraph{Buddy Allocator: Interface}
\ptitle{\code{block\_alloc(order)}} %ChkTex 36
\begin{enumerate}
    \item if no block of \code{order} is available, split higher order block recursively.
    \item allocate block of desired \code{order}
\end{enumerate}

Implementation:
\begin{lstlisting}[style=bright_C++]
struct block *buddy_alloc(unsigned order)
{
    if (order > BUDDY_MAX_ORDER) return NULL;
    int smallest_order = __buddy_find_smallest_free_order(order);
    if (smallest_order == -1) return -1;

    int ret = buddy_split(smallest_order, order);
    if(ret == -2) return NULL;

    return buddy_pop(order);
}
\end{lstlisting}

\newpar{}
\ptitle{\code{block\_free(block)}} %ChkTex 36
\begin{enumerate}
    \item free block
    \item if buddy is free, merge into larger block
\end{enumerate}

Implementation:
\begin{lstlisting}[style=bright_C++]
int buddy_free(struct block *block)
{
    switch (block->refcnt) {
    case 0:
        return -1; /* Double free */
    case 1:
        buddy_push(block, block->order);
        __buddy_try_merge(block);

        return 0;
    default:
        block->refcnt--;
        return 0;
    }
}
\end{lstlisting}

\textbf{Remark} both \code{block\_alloc} and \code{block\_free} trigger a max of log(\code{max\_order}) of operations. %ChkTeX 36

\paragraph{Buddy Allocator: Data Structures}
\ptitle{Block}
\begin{lstlisting}[style=bright_C++]
struct block {
    unsigned refcnt;
    unsigned order; /* 0 <= order <= BUDDY_MAX_ORDER */
    struct block *next;
};
\end{lstlisting}
\newpar{}
\begin{lstlisting}[style=bright_C++]
static struct block buddy_blocks[1UL << BUDDY_MAX_ORDER];
\end{lstlisting}

\ptitle{Free List}
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_free_lists[BUDDY_MAX_ORDER + 1];
\end{lstlisting}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item Static heap size assumed in Jake: \texttt{buddy\_blocks} in \texttt{.bss}
    \item This static assumption is not used in general OS (variable heap size per system)
    \begin{itemize}
        \item Use bootmem allocator
    \end{itemize}
    \item \texttt{buddy\_free\_lists} to avoid bitmap scans: cheap malloc
\end{itemize}

\paragraph{Buddy Allocator: Internal Functions}
\ptitle{Block to Buddy}
\begin{lstlisting}[style=bright_C++]
static inline struct block *block2buddy(struct block *block, int order)
{
    return __ppn2block(block2__ppn(block) ^ 1UL << order);
}
\end{lstlisting}
Note: If address instead of page number is used: flip \texttt{12+order+1}th bit.

\newpar{}
\ptitle{Smallest Order}
\begin{lstlisting}[style=bright_C++]
static int __buddy_find_smallest_free_order(unsigned order)
{
    for (int i = order; i <= BUDDY_MAX_ORDER; i++) {
        if (!is_list_empty(i)) return i;
    }
    return -1;
}
\end{lstlisting}

\newpar{}
\ptitle{Push}

Push to the front of \code{buddy\_free\_lists}
\begin{lstlisting}[style=bright_C++]
static void buddy_push(struct block *block, unsigned order)
{
	block->next = buddy_free_lists[order];
	buddy_free_lists[order] = block;
	block->refcnt--;
}
\end{lstlisting}

\newpar{}
\ptitle{Pop}

Pop to the front of \code{buddy\_free\_lists}
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_pop(unsigned order)
{
    if (is_list_empty(order)) return NULL;

    struct block *block = buddy_free_lists[order];

    /* update list and pop front of linked list */
	if (block->next != NULL) {
		buddy_free_lists[order] = block->next;
		block->next = NULL;
	} else {
		buddy_free_lists[order] = NULL;
	}
	block->refcnt++;

    return block;
}
\end{lstlisting}

\newpar{}
\ptitle{Remove}

Find \code{block} in  \code{buddy\_free\_lists} and \code{pop}.
\begin{lstlisting}[style=bright_C++]
static struct block *buddy_remove(struct block *block, unsigned order)
{
	if (is_list_empty(order)) return NULL;

	/* Find block in linked list and bring it to the front */
	if (buddy_free_lists[order] != block) {
		struct block *head = buddy_free_lists[order];
		struct block *tail = head;
		struct block *prev = NULL;

		while (tail->next) {
			if (tail->next == block) prev = tail;
			tail = tail->next;
		}

		if (!prev) return NULL; /* Could not find block */

		buddy_free_lists[order] = block;
		prev->next = NULL;
		tail->next = head;
	}

	return buddy_pop(order);
}
\end{lstlisting}

\newpar{}
\ptitle{Split}
\begin{lstlisting}[style=bright_C++]
static int buddy_split(unsigned smallest_free_order, unsigned desired_order)
{
	/* Will do nothing if smallest_free and desired are equal */
	for (int i = smallest_free_order; i > desired_order; i--) {
		int ret = __buddy_split(i);
		if (ret) return ret;
	}
	return 0;
}
\end{lstlisting}

\newpar{}
\begin{lstlisting}[style=bright_C++]
static int __buddy_split(unsigned order)
{
    if (order == 0) return -1;

    struct block *block = buddy_pop(order);

    if (!block) return -2; // no block available

    /* create buddy */
    block->order--;
    struct block *buddy = block2buddy(block, block->order);
    buddy->order = block->order;

    /* push split blocks to free lists */
    buddy_push(block, block->order);
    buddy_push(buddy, buddy->order);

    return 0;
}
\end{lstlisting}

\newpar{}
\ptitle{Merge}
\begin{lstlisting}[style=bright_C++]
static void __buddy_try_merge(struct block *block)
{
    if (block->order == BUDDY_MAX_ORDER) return;

    /* Get buddy of block */
    struct block *buddy = block2buddy(block, block->order);

    ////////////////////
    // TBD: how could this happen?
    ////////////////////
    if (block->order != buddy->order) return;

    if (is_block_free(block) && is_block_free(buddy)) {
        block = __buddy_merge(block, buddy);
        __buddy_try_merge(block);
    }
}
\end{lstlisting}
\newpar{}
\begin{lstlisting}[style=bright_C++]
static struct block *__buddy_merge(struct block *block, struct block *buddy)
{
    /* Remove them from current order's list */
    buddy_remove(buddy, buddy->order);
    buddy_remove(block, block->order);

    /* Merge into the one with the smaller address */
    if (block2__ppn(block) < block2__ppn(buddy)) {
        block->order++;
        buddy_push(block, block->order);
        return block;
    } else {
        buddy->order++;
        buddy_push(buddy, buddy->order);
        return buddy;
    }
}
\end{lstlisting}

\subsection{Virtual Memory}
% Segmentation and x86 history left out on purpose
With virtual memory, the users contiguous memory is spread over in physical memory to improve flexibility and external fragmentation.
In order to achieve this, the memory management unit (\textbf{MMU}) translates memory addresses accordingly to create the illusion of contiguous memory for the user. In addition the MMU also controls the permissions to that virtual memory space.
\subsubsection{Segmentation}

\begin{itemize}
    \item Inflexible
    \item Suffers from both internal and external fragmentation
\end{itemize}

\subsubsection{Paging}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_virt_mem.png}
\end{center}

\ptitle{Page Table}
\begin{itemize}
    \item translates the virtual page into a physical one
    \item In RISC-V, Pages have 4 KB (12 bit) granularity
    \item Kernel allocates pages (buddy)
    \item CPU's MMU translates addresses using the page table
\end{itemize}

\paragraph{Linear Page Table}
\begin{itemize}
    \item stored in CPU
    \item large unused parts (virtual $>>$ physical)
    \item simple
\end{itemize}

\ptitle{Dimensions}
\noindent\begin{equation*}
    s_{\text{page table}} = n_{\text{virt}}\cdot (\log_2(n_{\text{phys}}) + 1)\quad [\text{bits}]
\end{equation*}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}
\begin{tabularx}{\linewidth}{@{} c c c@{}}
                                                                                   & physical page                  & present bit \\
    \cmidrule{2-3}
    \multirow{7}{*}{\begin{sideways}$n_{\text{virt}}$\end{sideways}} & 10                             & 1           \\
                                                                                   & 01                             & 1           \\
                                                                                   & x                              & 0           \\
                                                                                   & x                              & 0           \\
                                                                                   & 00                             & 1           \\
                                                                                   & 11                             & 1           \\
                                                                                   & x                              & 0           \\
                                                                                   & x                              & 0           \\
    \cmidrule{2-3}
                                                                                   & $\log_2(n_{\text{phys}})$ bits &
\end{tabularx}

\paragraph{Multi-level Page Tables}
\begin{itemize}
    \item Pages are allocated when needed
    \item Stored im DRAM
    \item In our case (\textbf{Sv48}):
          \begin{itemize}
              \item 48 bit virtual and 56 bit physical address space
              \item 4 levels
              \item page table entry (PTE) is 8 bytes (64 bit)
              \item page is 4 KB $\to \frac{\text{4 KB}}{\text{8 B / entry}} = 512$ entries
              \begin{itemize}
                \item $\log_{2}(512) = 9$ bits required to index page table to get a PTE
              \end{itemize}
              \item virtual page number: 4 (levels) $\cdot$ 9 bits = 36 bits
              \begin{itemize}
                \item the remaining $48-36=12$ bits are used to index the ${2}^{12} = 4$ KB of physical bytes
              \end{itemize}
          \end{itemize}
\end{itemize}

\begin{center}
    \includegraphics[width =\linewidth]{kernel_sv48.png}
\end{center}

\ptitle{Virtual $\to$ physical (memory walk)}
\begin{itemize}
    \item Done by CPU's MMU
\end{itemize}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_ml_mem_walk.png}
\end{center}

\newpar{}
\ptitle{Creation}
\begin{itemize}
    \item Done in software (by OS)
    \item Identical to memory walk.
    \item If \code{present == 0}, allocate new page (\code{buddy\_alloc(0)} $\to$ \code{ppn}) and update PPN.\ %ChkTeX 36
\end{itemize}

\ptitle{Page Table Entry}

\begin{center}
    \includegraphics[width = \linewidth]{kernel_page_entry.png}
\end{center}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}llcll@{}}
    V & valid     &  & G    & accessed \\
    R & read      &  & D    & dirty    \\
    W & write     &  & PBMT & IO       \\
    X & execute   &  & N    & TLBOPT   \\
    U & user mode &  &
\end{tabularx}

\begin{tabularx}{\linewidth}{@{}cccl@{}}
    X & W & R & Meaning                             \\
    \cmidrule{1-4}
    0 & 0 & 0 & Pointer to next level of page table \\
    0 & 0 & 1 & Read-only page                      \\
    0 & 1 & 0 & Reserved for future use             \\
    0 & 1 & 1 & Read-write page                     \\
    1 & 0 & 0 & Execute-only page                   \\
    1 & 0 & 1 & Read-execute page                   \\
    1 & 1 & 0 & Reserved for future use             \\
    1 & 1 & 1 & Read-write-execute page
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\newpar{}
\ptitle{satp}
\begin{itemize}
    \item \code{satp} (supervisor address translation and protection) register points to the root of the page table
\end{itemize}
\begin{center}
    \includegraphics[width = \linewidth]{kernel_satp.png}
\end{center}
\begin{center}
    \includegraphics[width = .6\linewidth]{kernel_satp_table.png}
\end{center}

\newpar{}
\ptitle{Bigger Page Types}

Every memory access requires 4 additional lookups (memory accesses). If the applications only need bigger junks of memory then the lookup tree can be pruned by a page (or more).

\newpar{}
One page table has 512 entries ($2^9$).
\begin{align*}
    \text{page}       & = & 4\text{~KB}         &   &               \\
    \text{Huge page}  & = & 512 * 4\text{~KB}   & = & 2\text{~MB}   \\
    \text{Super page} & = & 512^2 * 4\text{~KB} & = & 1\text{~GB}   \\
    \text{Giant page} & = & 512^3 * 4\text{~KB} & = & 512\text{~GB}
\end{align*}

To determine if the current page table entry points to a next page table or to a bigger page size the X, W, R flags in are used (all 0 = next level of page table). In case of e.g.\ a huge page the \code{V} bit is set at level 1 already instead of level 0.

\paragraph{The MMU}
\begin{itemize}
    \item The MMU is a HW component
    \item After turning the MMU on, it interposes on each memory access:
    \begin{itemize}
        \item Go to the DRAM page at (physical) address stored in \code{satp}
        \item Execute a memory walk to the physical address that corresponds to the given virtual address
    \end{itemize}
    \item During the memory walk the MMU
    \begin{itemize}
        \item Checks:
        \begin{itemize}
            \item validity using the \code{V} bit
            \item permissions depending on the \code{R/W/X} bits
            \item kernel/user permissions depending on the \code{U} bit
            \begin{itemize}
                \item \texttt{sstatus} stores processor's state (kernel/user)
            \end{itemize}
        \end{itemize}
        \item Raises exceptions if validity or permissions are not given
    \end{itemize}
    \item Note that the kernel executes a similar PT walk in software when setting up/manipulating PTEs
\end{itemize}

\subsubsection{Virtual Memory Management}
The virtual memory space is divided into two sections one for the application and one for the kernel.
\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_app_kernel_space.png}
\end{center}

The kernel memory space contains five regions:

\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_kernel_space.png}
\end{center}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}lXccc@{}}
    Section        & Description                         & X & W & R \\
    \midrule
    \texttt{I/O}   & Interaction with devices            & 0 & 1 & 1 \\
    \texttt{.text} & Kernel code                         & 1 & 0 & 0 \\
    \texttt{.data} & Initialized global/static variables & 0 & 1 & 1 \\
    \texttt{ID}    & Identity map of physical memory     & 0 & 1 & 1 \\
    \texttt{stack} & Kernel stack                        & 0 & 1 & 1
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\paragraph{MMU Start-Up}

For backward compatibility reasons the kernel is located in the upper part of the virtual memory. This introduces a problem with the PC when the MMU starts. First the PC is pointing to the physical address of the kernel code. But after the MMU initialized the virtual memory space the PC points to the wrong address.

\begin{center}
    \includegraphics[width=\linewidth]{virtual_memory_mmu_problem.png}
\end{center}

This is tackled by
\begin{enumerate}
    \item Double mapping the kernel before the MMU starts.
    \item Update pointers (e.g., stack) and JALR.\
    \item Remove the double mapped kernel.
\end{enumerate}

\subsubsection{Page Fault}
If an application violates the permissions or tries to access an unmapped memory area the CPU throws an exception. (For more details on exceptions see Section~\ref{exceptions}) This exception is called a page fault.

There are two types of page faults:
\begin{itemize}
    \item \textbf{Hard page faults} are caused by invalid memory access due to a software bug. This results in the termination of the application.
    \item \textbf{Soft page faults} occur when valid memory is accessed with an invalid page table entry. In this case a new page table entry needs to be created (see~\ref{page demanding} Demand Paging).
\end{itemize}

\paragraph{Demand Paging}\label{page demanding}
Applications usually ask for more heap memory than they need so the heap memory is allocated when it is actually needed.
When an application accesses a not allocated but valid area in the memory we get a \textbf{soft page fault}. To determine the valid areas the kernel has a list of \textbf{virtual memory areas} (VMAs) for each process (and itself).
\newpar{}
Example procedure
\begin{enumerate}
    \item Application tries to store data
          \texttt{sw t0, (addr)}
    \item The MMU walks the pages and gets to an entry with the valid bit set to zero.
    \item The kernel checks if the requested address is in the valid area (VMA) and if the permissions are given.
    \item Then the page gets allocated.
    \item The store instruction is re-executed.
\end{enumerate}

\newpar{}
\ptitle{Demand Paging All The Way}

This dynamic approach can be used for all the app memory so that we do not have to pre-allocate any page tables.

\paragraph{Page Sharing}
% TODO:
\paragraph{Page Swapping}
% TODO:

\subsection{Exceptions}\label{exceptions}
An exception in the pipeline can either be generated
\begin{itemize}
    \item \textbf{internally} by
          \begin{itemize}
              \item incorrect behavior like executing an invalid instruction.
              \item a potentially valid behavior like page faults.
          \end{itemize}
    \item \textbf{externally} by
          \begin{itemize}
              \item interrupts like a network packet arrival or a key press.
          \end{itemize}
\end{itemize}

Exceptions are handled by flushing the pipeline and moving control to a pre-configured OS routine.

\begin{center}
    \includegraphics[width=\linewidth]{exception_handling.png}
\end{center}

\newpar{}
\ptitle{STVEC} Supervisor Trap Vector register

On an exception the CPU jumps to the address stored in STVEC

\newpar{}
\ptitle{SEPC} Supervisor Exception Program Counter register

SEPC will hold the PC of the offending instruction (to be used by the routine at STVEC)

\newpar{}
\ptitle{SCAUSE} Supervisor Exception CAUSE register

SCAUSE will tell why the exception happened (e.g., a page fault)

\newpar{}
\ptitle{STVAL} Supervisor Trap Value register

STVAL will provide extended information about the exception (e.g., page fault's address)

\newpar{}
\ptitle{Example}

On a soft PF the CPU jumps to STVEC. The handler routine sees which instruction caused the PF (the instruction at SEPC), knows from SCAUSE that the reason was a PF and finds the (user mode virtual) address (page number) at which the exception occured in STVAL. A routine can then be called to load the missing code page to the address stored in STVAL.
% TODO: how is this not the exact same as above?